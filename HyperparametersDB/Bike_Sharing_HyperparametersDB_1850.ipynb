{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h2o package and specific estimator \n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "import random, os, sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "import optparse\n",
    "import time\n",
    "import json\n",
    "from distutils.util import strtobool\n",
    "import psutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(strict_version_check=False) # start h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data to the server\n",
    "hp = h2o.import_file(path=\"hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the head\n",
    "hp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def alphabet(n):\n",
    "  alpha='0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'    \n",
    "  str=''\n",
    "  r=len(alpha)-1   \n",
    "  while len(str)<n:\n",
    "    i=random.randint(0,r)\n",
    "    str+=alpha[i]   \n",
    "  return str\n",
    "  \n",
    "  \n",
    "def set_meta_data(analysis,run_id,server,data,test,model_path,target,run_time,classification,scale,model,balance,balance_threshold,name,path,nthreads,min_mem_size):\n",
    "  m_data={}\n",
    "  m_data['start_time'] = time.time()\n",
    "  m_data['target']=target\n",
    "  m_data['server_path']=server\n",
    "  m_data['data_path']=data \n",
    "  m_data['test_path']=test\n",
    "  m_data['max_models']=model\n",
    "  m_data['run_time']=run_time\n",
    "  m_data['run_id'] =run_id\n",
    "  m_data['scale']=scale\n",
    "  m_data['classification']=classification\n",
    "  m_data['scale']=False\n",
    "  m_data['model_path']=model_path\n",
    "  m_data['balance']=balance\n",
    "  m_data['balance_threshold']=balance_threshold\n",
    "  m_data['project'] =name\n",
    "  m_data['end_time'] = time.time()\n",
    "  m_data['execution_time'] = 0.0\n",
    "  m_data['run_path'] =path\n",
    "  m_data['nthreads'] = nthreads\n",
    "  m_data['min_mem_size'] = min_mem_size\n",
    "  m_data['analysis'] = analysis\n",
    "  return m_data\n",
    "\n",
    "\n",
    "def dict_to_json(dct,n):\n",
    "  j = json.dumps(dct, indent=4)\n",
    "  f = open(n, 'w')\n",
    "  print(j, file=f)\n",
    "  f.close()\n",
    "  \n",
    "  \n",
    "def stackedensemble(mod):\n",
    "    coef_norm=None\n",
    "    try:\n",
    "      metalearner = h2o.get_model(mod.metalearner()['name'])\n",
    "      coef_norm=metalearner.coef_norm()\n",
    "    except:\n",
    "      pass        \n",
    "    return coef_norm\n",
    "\n",
    "def stackedensemble_df(df):\n",
    "    bm_algo={ 'GBM': None,'GLM': None,'DRF': None,'XRT': None,'Dee': None}\n",
    "    for index, row in df.iterrows():\n",
    "      if len(row['model_id'])>3:\n",
    "        key=row['model_id'][0:3]\n",
    "        if key in bm_algo:\n",
    "          if bm_algo[key] is None:\n",
    "                bm_algo[key]=row['model_id']\n",
    "    bm=list(bm_algo.values()) \n",
    "    bm=list(filter(None.__ne__, bm))             \n",
    "    return bm\n",
    "\n",
    "def se_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['auc']=modl.auc()   \n",
    "    d['roc']=modl.roc()\n",
    "    d['mse']=modl.mse()   \n",
    "    d['null_degrees_of_freedom']=modl.null_degrees_of_freedom()\n",
    "    d['null_deviance']=modl.null_deviance()\n",
    "    d['residual_degrees_of_freedom']=modl.residual_degrees_of_freedom()   \n",
    "    d['residual_deviance']=modl.residual_deviance()\n",
    "    d['rmse']=modl.rmse()\n",
    "    return d\n",
    "\n",
    "def get_model_by_algo(algo,models_dict):\n",
    "    mod=None\n",
    "    mod_id=None    \n",
    "    for m in list(models_dict.keys()):\n",
    "        if m[0:3]==algo:\n",
    "            mod_id=m\n",
    "            mod=h2o.get_model(m)      \n",
    "    return mod,mod_id     \n",
    "    \n",
    "    \n",
    "def gbm_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['varimp']=modl.varimp()  \n",
    "    return d\n",
    "    \n",
    "    \n",
    "def dl_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['varimp']=modl.varimp()  \n",
    "    return d\n",
    "    \n",
    "    \n",
    "def drf_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['varimp']=modl.varimp()  \n",
    "    d['roc']=modl.roc()      \n",
    "    return d\n",
    "    \n",
    "def xrt_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['varimp']=modl.varimp()  \n",
    "    d['roc']=modl.roc()      \n",
    "    return d\n",
    "    \n",
    "    \n",
    "def glm_stats(modl):\n",
    "    d={}\n",
    "    d['algo']=modl.algo\n",
    "    d['model_id']=modl.model_id   \n",
    "    d['coef']=modl.coef()  \n",
    "    d['coef_norm']=modl.coef_norm()      \n",
    "    return d\n",
    "    \n",
    "def model_performance_stats(perf):\n",
    "    d={}\n",
    "    try:    \n",
    "      d['mse']=perf.mse()\n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['rmse']=perf.rmse() \n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['null_degrees_of_freedom']=perf.null_degrees_of_freedom()\n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['residual_degrees_of_freedom']=perf.residual_degrees_of_freedom()\n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['residual_deviance']=perf.residual_deviance() \n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['null_deviance']=perf.null_deviance() \n",
    "    except:\n",
    "      pass      \n",
    "    try:    \n",
    "      d['aic']=perf.aic() \n",
    "    except:\n",
    "      pass      \n",
    "    try:\n",
    "      d['logloss']=perf.logloss() \n",
    "    except:\n",
    "      pass    \n",
    "    try:\n",
    "      d['auc']=perf.auc()\n",
    "    except:\n",
    "      pass  \n",
    "    try:\n",
    "      d['gini']=perf.gini()\n",
    "    except:\n",
    "      pass    \n",
    "    return d\n",
    "    \n",
    "def impute_missing_values(df, x, scal=False):\n",
    "    # determine column types\n",
    "    ints, reals, enums = [], [], []\n",
    "    for key, val in df.types.items():\n",
    "        if key in x:\n",
    "            if val == 'enum':\n",
    "                enums.append(key)\n",
    "            elif val == 'int':\n",
    "                ints.append(key)            \n",
    "            else: \n",
    "                reals.append(key)    \n",
    "    _ = df[reals].impute(method='mean')\n",
    "    _ = df[ints].impute(method='median')\n",
    "    if scal:\n",
    "        df[reals] = df[reals].scale()\n",
    "        df[ints] = df[ints].scale()    \n",
    "    return\n",
    "\n",
    "\n",
    "def get_independent_variables(df, targ):\n",
    "    C = [name for name in df.columns if name != targ]\n",
    "    # determine column types\n",
    "    ints, reals, enums = [], [], []\n",
    "    for key, val in df.types.items():\n",
    "        if key in C:\n",
    "            if val == 'enum':\n",
    "                enums.append(key)\n",
    "            elif val == 'int':\n",
    "                ints.append(key)            \n",
    "            else: \n",
    "                reals.append(key)    \n",
    "    x=ints+enums+reals\n",
    "    return x\n",
    "    \n",
    "def get_all_variables_csv(i):\n",
    "    ivd={}\n",
    "    try:\n",
    "      iv = pd.read_csv(i,header=None)\n",
    "    except:\n",
    "      sys.exit(1)    \n",
    "    col=iv.values.tolist()[0]\n",
    "    dt=iv.values.tolist()[1]\n",
    "    i=0\n",
    "    for c in col:\n",
    "      ivd[c.strip()]=dt[i].strip()\n",
    "      i+=1        \n",
    "    return ivd\n",
    "    \n",
    "    \n",
    "\n",
    "def check_all_variables(df,dct,y=None):     \n",
    "    targ=list(dct.keys())     \n",
    "    for key, val in df.types.items():\n",
    "        if key in targ:\n",
    "          if dct[key] not in ['real','int','enum']:                      \n",
    "            targ.remove(key)  \n",
    "    for key, val in df.types.items():\n",
    "        if key in targ:            \n",
    "          if dct[key] != val:\n",
    "            print('convert ',key,' ',dct[key],' ',val)\n",
    "            if dct[key]=='enum':\n",
    "                try:\n",
    "                  df[key] = df[key].asfactor() \n",
    "                except:\n",
    "                  targ.remove(key)                 \n",
    "            if dct[key]=='int': \n",
    "                try:                \n",
    "                  df[key] = df[key].asnumeric() \n",
    "                except:\n",
    "                  targ.remove(key)                  \n",
    "            if dct[key]=='real':\n",
    "                try:                \n",
    "                  df[key] = df[key].asnumeric()  \n",
    "                except:\n",
    "                  targ.remove(key)                  \n",
    "    if y is None:\n",
    "      y=df.columns[-1] \n",
    "    if y in targ:\n",
    "      targ.remove(y)\n",
    "    else:\n",
    "      y=targ.pop()            \n",
    "    return targ    \n",
    "    \n",
    "def predictions(mod,data,run_id):\n",
    "    test = h2o.import_file(data)\n",
    "    mod_perf=mod_best.model_performance(test)\n",
    "              \n",
    "    stats_test={}\n",
    "    stats_test=model_performance_stats(mod_perf)\n",
    "\n",
    "    n=run_id+'_test_stats.json'\n",
    "    dict_to_json(stats_test,n) \n",
    "\n",
    "    try:    \n",
    "      cf=mod_perf.confusion_matrix(metrics=[\"f1\",\"f2\",\"f0point5\",\"accuracy\",\"precision\",\"recall\",\"specificity\",\"absolute_mcc\",\"min_per_class_accuracy\",\"mean_per_class_accuracy\"])\n",
    "      cf_df=cf[0].table.as_data_frame()\n",
    "      cf_df.to_csv(run_id+'_test_confusion_matrix.csv')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    predictions = mod_best.predict(test)\n",
    "    predictions_df=test.cbind(predictions).as_data_frame() \n",
    "    predictions_df.to_csv(run_id+'_predictions.csv')\n",
    "    return\n",
    "\n",
    "def predictions_test(mod,test,run_id):\n",
    "    mod_perf=mod_best.model_performance(test)          \n",
    "    stats_test={}\n",
    "    stats_test=model_performance_stats(mod_perf)\n",
    "    n=run_id+'_test_stats.json'\n",
    "    dict_to_json(stats_test,n) \n",
    "    try:\n",
    "      cf=mod_perf.confusion_matrix()\n",
    "#      cf=mod_perf.confusion_matrix(metrics=[\"f1\",\"f2\",\"f0point5\",\"accuracy\",\"precision\",\"recall\",\"specificity\",\"absolute_mcc\",\"min_per_class_accuracy\",\"mean_per_class_accuracy\"])\n",
    "      cf_df=cf.table.as_data_frame()\n",
    "      cf_df.to_csv(run_id+'_test_confusion_matrix.csv')\n",
    "    except:\n",
    "      pass\n",
    "    predictions = mod_best.predict(test)    \n",
    "    predictions_df=test.cbind(predictions).as_data_frame() \n",
    "    predictions_df.to_csv(run_id+'_predictions.csv')\n",
    "    return predictions\n",
    "\n",
    "def check_X(x,df):\n",
    "    for name in x:\n",
    "        if name not in df.columns:\n",
    "          x.remove(name)  \n",
    "    return x    \n",
    "    \n",
    "    \n",
    "def get_stacked_ensemble(lst):\n",
    "    se=None\n",
    "    for model in model_set:\n",
    "      if 'BestOfFamily' in model:\n",
    "        se=model\n",
    "    if se is None:     \n",
    "      for model in model_set:\n",
    "        if 'AllModels'in model:\n",
    "          se=model           \n",
    "    return se       \n",
    "    \n",
    "def get_variables_types(df):\n",
    "    d={}\n",
    "    for key, val in df.types.items():\n",
    "        d[key]=val           \n",
    "    return d    \n",
    "    \n",
    "#  End Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with runtime 1850 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following are passed by the user from the web interface\n",
    "\n",
    "'''\n",
    "Need a user id and project id?\n",
    "\n",
    "'''\n",
    "target='cnt' \n",
    "data_file='hour.csv'\n",
    "run_time=1850\n",
    "run_id='SOME_ID_20180617_221531' # Just some arbitrary ID\n",
    "server_path='Users/newzysharma/Desktop/Desktop/Machine_Learning/Project'\n",
    "classification=False\n",
    "scale=False\n",
    "max_models=None\n",
    "balance_y=False # balance_classes=balance_y\n",
    "balance_threshold=0.2\n",
    "project =\"HyperparameterDB_Project\"  # project_name = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "import os\n",
    "\n",
    "data_path=os.path.join(server_path,data_file)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign target and inputs for logistic regression\n",
    "y = target\n",
    "X = [name for name in hp.columns if name != y]\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "_ = hp[reals].impute(method='mean')\n",
    "_ = hp[ints].impute(method='median')\n",
    "\n",
    "if scale:\n",
    "    hp[reals] = hp[reals].scale()\n",
    "    hp[ints] = hp[ints].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classification:\n",
    "    class_percentage = y_balance=df[y].mean()[0]/(df[y].max()-df[y].min())\n",
    "    if class_percentage < balance_threshold:\n",
    "        balance_y=True\n",
    "        \n",
    "\n",
    "print(run_time)\n",
    "type(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate rather than take a test training split with 1850 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automl\n",
    "# runs for run_time seconds then builds a stacked ensemble\n",
    "\n",
    "aml = H2OAutoML(max_runtime_secs=run_time,project_name = project) # init automl, run for 1850 seconds\n",
    "aml.train(x=X,  \n",
    "           y=y,\n",
    "           training_frame=hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "aml_leaderboard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for GBM_2_AutoML_20190408_171007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "model_set=aml_leaderboard_df['model_id']\n",
    "mod_best=h2o.get_model(model_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_best.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for XGBoost_1_AutoML_20190408_190707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "model_set=aml_leaderboard_df['model_id']\n",
    "mod_best=h2o.get_model(model_set[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_best.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for StackedEnsemble_AllModels_AutoML_20190408_171007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "model_set=aml_leaderboard_df['model_id']\n",
    "mod_best=h2o.get_model(model_set[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_best.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
